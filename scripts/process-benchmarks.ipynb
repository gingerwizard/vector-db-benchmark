{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to process benchmar results\n",
    "\n",
    "Please run this notebook after running all the benchmarks and storing them in the `results` dir. This will export them in the desired format for the single node benchmark plots of [qdrant.tech/benchmarks](https://qdrant.tech/benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-10T13:27:56.803970Z",
     "start_time": "2024-01-10T13:27:56.157684Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-10T13:27:58.876612Z",
     "start_time": "2024-01-10T13:27:58.863557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(PosixPath('/opt/vector-db-benchmark/results'),\n 'clickhouse-default-glove-100-angular-upload-2024-01-10-13-14-57.json')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = Path().resolve().parent / \"results\"\n",
    "DATA_DIR, list(DATA_DIR.glob(\"*.json\"))[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-10T13:59:16.542966Z",
     "start_time": "2024-01-10T13:59:16.531815Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_REGEX = re.compile(r\"(?P<engine_name>(?P<engine>[a-z]+)(?:-default)?\"\n",
    "                        r\"(?:\\-m\\-(?P<m>[0-9]+))?(?:\\-ef\\-(?P<ef>[0-9]+))?)\"\n",
    "                        r\"\\-(?P<dataset>[a-zA-Z0-9\\-]+)\\-(?P<operation>(search)|(upload))\"\n",
    "                        r\"(\\-(?P<search_index>[0-9]{1,2})\\-)?\\-?(?P<date>.*)\\.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-10T13:59:18.737700Z",
     "start_time": "2024-01-10T13:59:18.710922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 5)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_results, search_results = [], []\n",
    "\n",
    "for path in DATA_DIR.glob(\"*.json\"):\n",
    "    match = PATH_REGEX.match(path.name)\n",
    "    if match is None:\n",
    "        continue\n",
    "        \n",
    "    experiment = match.groupdict()\n",
    "    \n",
    "    with open(path, \"r\") as fp:\n",
    "        stats = json.load(fp)\n",
    "\n",
    "    entry = [match[\"engine\"], match[\"m\"], match[\"ef\"], \n",
    "             match[\"dataset\"], match[\"search_index\"], match[\"date\"], \n",
    "             stats[\"params\"], stats[\"results\"]]\n",
    "    if experiment[\"operation\"] == \"search\":\n",
    "        search_results.append(entry)\n",
    "    elif experiment[\"operation\"] == \"upload\":\n",
    "        upload_results.append(entry)\n",
    "\n",
    "len(upload_results), len(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-10T13:59:21.185131Z",
     "start_time": "2024-01-10T13:59:21.177801Z"
    }
   },
   "outputs": [],
   "source": [
    "column_names = [\"engine\", \"m\", \"ef\", \"dataset\", \"search_index\", \"date\", \"params\", \"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-10T13:59:24.269816Z",
     "start_time": "2024-01-10T13:59:24.157559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'total_time'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/48/dc999rwj32s3nv1pkl1sdq4w0000gn/T/ipykernel_81975/1197450097.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mupload_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mupload_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"results\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mupload_df\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m \u001B[0mupload_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msort_values\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"total_time\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mascending\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/vector-db-benchmark-R9m_yuaJ-py3.11/lib/python3.11/site-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001B[0m\n\u001B[1;32m   6754\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mby\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6755\u001B[0m             \u001B[0;31m# len(by) == 1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6756\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6757\u001B[0m             \u001B[0mby\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mby\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 6758\u001B[0;31m             \u001B[0mk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_label_or_level_values\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mby\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   6759\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6760\u001B[0m             \u001B[0;31m# need to rewrap column in Series to apply key function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6761\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/vector-db-benchmark-R9m_yuaJ-py3.11/lib/python3.11/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1774\u001B[0m             \u001B[0mvalues\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mxs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mother_axes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1775\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_level_reference\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1776\u001B[0m             \u001B[0mvalues\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maxes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_level_values\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1777\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1778\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1779\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1780\u001B[0m         \u001B[0;31m# Check for duplicates\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1781\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mvalues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'total_time'"
     ]
    }
   ],
   "source": [
    "upload_df = pd.DataFrame(upload_results, columns=column_names) \\\n",
    "    .drop(columns=\"search_index\")\n",
    "upload_df[\"date\"] = pd.to_datetime(upload_df[\"date\"], format=\"%Y-%m-%d-%H-%M-%S\")\n",
    "upload_df = upload_df.sort_values(\"date\", ascending=False) \\\n",
    "    .groupby([\"engine\", \"m\", \"ef\", \"dataset\"]) \\\n",
    "    .last()\n",
    "upload_df = pd.concat([upload_df, upload_df[\"results\"].apply(pd.Series)], axis=1)\n",
    "upload_df = upload_df.drop(columns=\"results\")\n",
    "\n",
    "print(len(upload_df))\n",
    "\n",
    "upload_df.sort_values(\"total_time\", ascending=True).head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-05T12:06:58.459530Z",
     "start_time": "2022-08-05T12:06:57.908842Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "search_df = pd.DataFrame(search_results, columns=column_names)\n",
    "search_df[\"date\"] = pd.to_datetime(search_df[\"date\"], format=\"%Y-%m-%d-%H-%M-%S\")\n",
    "search_df = search_df.sort_values(\"date\", ascending=False) \\\n",
    "    .groupby([\"engine\", \"m\", \"ef\", \"dataset\", \"search_index\"]) \\\n",
    "    .first()\n",
    "\n",
    "print(len(search_df))\n",
    "\n",
    "for column_name in [\"params\", \"results\"]:\n",
    "    search_df = pd.concat([search_df, search_df[column_name].apply(pd.Series)], axis=1)\n",
    "    search_df = search_df.drop(columns=column_name)\n",
    "search_df.sort_values(\"rps\", ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_search = search_df.reset_index()\n",
    "_upload = upload_df.reset_index()\n",
    "\n",
    "joined_df = _search.merge(_upload, on=[\"engine\", \"m\", \"ef\", \"dataset\"], how=\"left\", suffixes=(\"_search\", \"_upload\"))\n",
    "print(len(joined_df))\n",
    "joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_all = []\n",
    "json_1_or_100_thread = []\n",
    "\n",
    "for index, row in joined_df.reset_index().iterrows():\n",
    "    engine_params = {}\n",
    "    if isinstance(row['search_params'], dict):\n",
    "        engine_params.update(row['search_params'])\n",
    "    if isinstance(row['params'], dict):\n",
    "        engine_params.update(row['params'])\n",
    "\n",
    "    engine_name = row['engine']\n",
    "\n",
    "    if engine_name == \"qdrant-rps\" or engine_name == \"qdrant-bq-rps\" or engine_name == \"qdrant-sq-rps\":\n",
    "        engine_name = \"qdrant\"\n",
    "\n",
    "    json_object = {\n",
    "        \"engine_name\": engine_name,\n",
    "        \"setup_name\": f\"{row['engine']}-m-{row['m']}-ef-{row['ef']}\",\n",
    "        \"dataset_name\": row['dataset'],\n",
    "        # \"search_idx\": row['search_index'],\n",
    "        \"upload_time\": row['upload_time'],\n",
    "        \"total_upload_time\": row['total_time_upload'],\n",
    "        \"p95_time\": row['p95_time'],\n",
    "        \"rps\": row['rps'],\n",
    "        \"parallel\": row['parallel'],\n",
    "        \"p99_time\": row['p99_time'],\n",
    "        \"mean_time\": row['mean_time'],\n",
    "        \"mean_precisions\": row['mean_precisions'],\n",
    "        \"engine_params\": engine_params,\n",
    "    }\n",
    "    json_all.append(json_object)\n",
    "    \n",
    "    parallel = row['parallel']\n",
    "\n",
    "    if parallel == 1 or parallel == 100:\n",
    "        json_1_or_100_thread.append(json_object)\n",
    "\n",
    "format = '%Y-%M-%d' # T%H:%M:%S\n",
    "now = datetime.now().replace(tzinfo=timezone.utc).strftime(format)\n",
    "\n",
    "Path(f\"results-{now}.json\").write_text(json.dumps(json_all, indent=2))\n",
    "Path(f\"results-1-100-threads-{now}.json\").write_text(json.dumps(json_1_or_100_thread, indent=2))\n",
    "\n",
    "json_1_or_100_thread[-1], len(json_all), len(json_1_or_100_thread)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
