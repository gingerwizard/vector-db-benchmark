{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to process benchmar results\n",
    "\n",
    "Please run this notebook after running all the benchmarks and storing them in the `results` dir. This will export them in the desired format for the single node benchmark plots of [qdrant.tech/benchmarks](https://qdrant.tech/benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-10T13:27:56.803970Z",
     "start_time": "2024-01-10T13:27:56.157684Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-10T13:27:58.876612Z",
     "start_time": "2024-01-10T13:27:58.863557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(PosixPath('/opt/vector-db-benchmark/results'),\n 'clickhouse-default-glove-100-angular-upload-2024-01-10-13-14-57.json')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = Path().resolve().parent / \"results\"\n",
    "DATA_DIR, list(DATA_DIR.glob(\"*.json\"))[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-10T13:59:16.542966Z",
     "start_time": "2024-01-10T13:59:16.531815Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_REGEX = re.compile(r\"(?P<engine_name>(?P<engine>[a-z]+)(?:-default)?\"\n",
    "                        r\"(?:\\-m\\-(?P<m>[0-9]+))?(?:\\-ef\\-(?P<ef>[0-9]+))?)\"\n",
    "                        r\"\\-(?P<dataset>[a-zA-Z0-9\\-]+)\\-(?P<operation>(search)|(upload))\"\n",
    "                        r\"(\\-(?P<search_index>[0-9]{1,2})\\-)?\\-?(?P<date>.*)\\.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-10T13:59:18.737700Z",
     "start_time": "2024-01-10T13:59:18.710922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 5)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_results, search_results = [], []\n",
    "\n",
    "for path in DATA_DIR.glob(\"*.json\"):\n",
    "    match = PATH_REGEX.match(path.name)\n",
    "    if match is None:\n",
    "        continue\n",
    "\n",
    "    experiment = match.groupdict()\n",
    "\n",
    "    with open(path, \"r\") as fp:\n",
    "        stats = json.load(fp)\n",
    "\n",
    "    params = stats[\"params\"]\n",
    "    dataset = params.pop(\"dataset\")\n",
    "    engine = params.pop(\"engine\")\n",
    "\n",
    "    entry = {\n",
    "        \"dataset\": dataset,\n",
    "        \"engine\": engine,\n",
    "        \"m\": match[\"m\"],\n",
    "        \"ef\": match[\"ef\"],\n",
    "        \"date\": match[\"date\"],\n",
    "        \"params\": params,\n",
    "        \"results\": stats[\"results\"],\n",
    "    }\n",
    "\n",
    "    if experiment[\"operation\"] == \"search\":\n",
    "        entry.update({\"search_index\": match[\"search_index\"]})\n",
    "        search_results.append(entry)\n",
    "    elif experiment[\"operation\"] == \"upload\":\n",
    "        upload_results.append(entry)\n",
    "    else:\n",
    "        raise Exception(\"Unknown operation\")\n",
    "\n",
    "len(upload_results), len(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-10T13:59:21.185131Z",
     "start_time": "2024-01-10T13:59:21.177801Z"
    }
   },
   "outputs": [],
   "source": [
    "upload_results, search_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-01-10T13:59:24.269816Z",
     "start_time": "2024-01-10T13:59:24.157559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'total_time'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/48/dc999rwj32s3nv1pkl1sdq4w0000gn/T/ipykernel_81975/1197450097.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mupload_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mupload_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"results\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mupload_df\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m \u001B[0mupload_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msort_values\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"total_time\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mascending\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/vector-db-benchmark-R9m_yuaJ-py3.11/lib/python3.11/site-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001B[0m\n\u001B[1;32m   6754\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mby\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6755\u001B[0m             \u001B[0;31m# len(by) == 1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6756\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6757\u001B[0m             \u001B[0mby\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mby\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 6758\u001B[0;31m             \u001B[0mk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_label_or_level_values\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mby\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   6759\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6760\u001B[0m             \u001B[0;31m# need to rewrap column in Series to apply key function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6761\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/vector-db-benchmark-R9m_yuaJ-py3.11/lib/python3.11/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1774\u001B[0m             \u001B[0mvalues\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mxs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mother_axes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1775\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_level_reference\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1776\u001B[0m             \u001B[0mvalues\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maxes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_level_values\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1777\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1778\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1779\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1780\u001B[0m         \u001B[0;31m# Check for duplicates\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1781\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mvalues\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'total_time'"
     ]
    }
   ],
   "source": [
    "upload_df = pd.DataFrame(upload_results)\n",
    "upload_df[\"date\"] = pd.to_datetime(upload_df[\"date\"], format=\"%Y-%m-%d-%H-%M-%S\")\n",
    "upload_df = upload_df.sort_values(\"date\", ascending=False) \\\n",
    "    .groupby([\"engine\", \"m\", \"ef\", \"dataset\"]) \\\n",
    "    .first()\n",
    "\n",
    "temp_df = upload_df.copy()\n",
    "temp_df[\"total_time\"] = temp_df[\"results\"].apply(lambda x: x[\"total_time\"])\n",
    "temp_df.sort_values(\"total_time\", ascending=True).head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-05T12:06:58.459530Z",
     "start_time": "2022-08-05T12:06:57.908842Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "search_df = pd.DataFrame(search_results)\n",
    "search_df[\"date\"] = pd.to_datetime(search_df[\"date\"], format=\"%Y-%m-%d-%H-%M-%S\")\n",
    "search_df = search_df.sort_values(\"date\", ascending=False) \\\n",
    "    .groupby([\"engine\", \"m\", \"ef\", \"dataset\", \"search_index\"]) \\\n",
    "    .first()\n",
    "\n",
    "temp_df = search_df.copy()\n",
    "temp_df['rps'] = temp_df['results'].apply(lambda x: x[\"rps\"])\n",
    "temp_df.sort_values(\"rps\", ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_search = search_df.reset_index()\n",
    "_upload = upload_df.reset_index()\n",
    "\n",
    "joined_df = _search.merge(_upload, on=[\"engine\", \"m\", \"ef\", \"dataset\"], how=\"left\", suffixes=(\"_search\", \"_upload\"))\n",
    "print(len(joined_df))\n",
    "joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_results = []\n",
    "\n",
    "for index, row in joined_df.reset_index().iterrows():\n",
    "    engine_params = {}\n",
    "    \n",
    "    if isinstance(row['params_upload'], dict):\n",
    "        engine_params.update(row['params_upload'])\n",
    "    if isinstance(row['params_search'], dict):\n",
    "        search_params = row['params_search']\n",
    "        engine_params.update(search_params.get('config', {}))\n",
    "        engine_params.update(search_params.get('params', {}))\n",
    "        engine_params.update(search_params.get('search_params', {}))\n",
    "        engine_params.update(search_params.get('vectorIndexConfig', {}))\n",
    "\n",
    "    engine_params.pop('experiment')\n",
    "    engine_params.pop('parallel')\n",
    "\n",
    "    engine_name = row['engine']\n",
    "\n",
    "    if engine_name.startswith(\"qdrant-\"):\n",
    "        engine_name = \"qdrant\"\n",
    "\n",
    "    json_object = {\n",
    "        \"engine_name\": engine_name,\n",
    "        \"setup_name\": f\"{row['params_search']['experiment']}\",\n",
    "        \"dataset_name\": row['dataset'],\n",
    "        \"search_idx\": row['search_index'],\n",
    "        \"upload_time\": row['results_upload']['upload_time'],\n",
    "        \"total_upload_time\": row['results_upload']['total_time'],\n",
    "        \"p95_time\": row['results_search']['p95_time'],\n",
    "        \"rps\": row['results_search']['rps'],\n",
    "        \"parallel\": row['params_search']['parallel'],\n",
    "        \"p99_time\": row['results_search']['p99_time'],\n",
    "        \"mean_time\": row['results_search']['mean_time'],\n",
    "        \"mean_precisions\": row['results_search']['mean_precisions'],\n",
    "        \"engine_params\": engine_params,\n",
    "    }\n",
    "    json_results.append(json_object)\n",
    "\n",
    "format = '%Y-%M-%dT%H:%M:%S'\n",
    "now = datetime.now().replace(tzinfo=timezone.utc).strftime(format)\n",
    "\n",
    "Path(f\"results.json\").write_text(json.dumps(json_results, indent=2))\n",
    "Path(f\"results-{now}.json\").write_text(json.dumps(json_results, indent=2))\n",
    "\n",
    "print(json_results[-1], len(json_results))\n",
    "\n",
    "results_df = pd.DataFrame(json_results).sort_values(\"p99_time\", ascending=True)\n",
    "# results_df.to_csv('results.csv')\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
